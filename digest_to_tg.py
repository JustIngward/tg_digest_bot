#!/usr/bin/env python3
"""ITâ€‘Digest Telegram bot â€” v1.0Â (2025â€‘04â€‘22, rewrite)

â–¶  ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ð¹ Â«Ñ Ð½ÑƒÐ»ÑÂ»
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–ªÂ Python ÑÐ¾Ð±Ð¸Ñ€Ð°ÐµÑ‚ Ð½Ð¾Ð²Ð¾ÑÑ‚Ð¸ (NewsAPI + RSS) â†’Â Ð¾Ð´Ð½Ð¾ Ð¾Ð±Ñ€Ð°Ñ‰ÐµÐ½Ð¸Ðµ Ðº GPTâ€‘4o
  Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÑ‚ Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹Ð¹ Markdownâ€‘Ð´Ð°Ð¹Ð´Ð¶ÐµÑÑ‚.
â–ªÂ Ð‘ÐµÐ· Ñ†Ð¸ÐºÐ»Ð¾Ð² toolâ€‘calling, Ð±ÐµÐ· ÑÐ»Ð¾Ð¶Ð½Ñ‹Ñ… regex: Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐºÐ¾Ð»â€‘Ð²Ð¾ Ð¿ÑƒÐ½ÐºÑ‚Ð¾Ð².
â–ªÂ SQLiteâ€‘Ð¿Ð°Ð¼ÑÑ‚ÑŒ Ð¾ÑÑ‚Ð°Ð²Ð¸Ð»Ð¸, Ð½Ð¾ Ð¼Ð¾Ð¶Ð½Ð¾ Ð²Ñ‹ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ `USE_DB=0`.
"""

from __future__ import annotations

import os, datetime as dt, json, sqlite3, textwrap, requests, feedparser
from urllib.parse import urlparse
from dotenv import load_dotenv
from openai import OpenAI

# â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€
load_dotenv()
TZ               = dt.timezone(dt.timedelta(hours=3))
MODEL            = os.getenv("MODEL", "gpt-4o")
OPENAI_KEY       = os.getenv("OPENAI_API_KEY")
NEWS_API_KEY     = os.getenv("NEWS_API_KEY")
TG_TOKEN         = os.getenv("TG_TOKEN")
CHAT_ID          = os.getenv("CHAT_ID")
MAX_DAYS         = int(os.getenv("MAX_DAYS", 10))
MAX_ARTICLES     = int(os.getenv("MAX_ARTICLES", 30))
DIGEST_NEWS_CNT  = int(os.getenv("DIGEST_NEWS_CNT", 12))  # Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ñ‹Ñ… ÑÑ‚Ñ€Ð¾Ðº
USE_DB           = int(os.getenv("USE_DB", 1))
SQLITE_PATH      = os.getenv("DB_PATH", "sent.db")

assert all([OPENAI_KEY, TG_TOKEN, CHAT_ID, NEWS_API_KEY]), "Missing env vars"

client = OpenAI()

WHITELIST = {
    "cnews.ru", "tadviser.ru", "vc.ru", "rbc.ru", "gazeta.ru",
    "1c.ru", "infostart.ru", "odysseyconsgroup.com",
    "rusbase.ru", "trends.rbc.ru"
}

RSS_FEEDS = [
    "https://habr.com/ru/rss/all/all/?fl=ru",
    "https://1c.ru/news/all.rss",
    "https://infostart.ru/rss/news/",
    "https://vc.ru/rss",
]

# â”€â”€â”€â”€â”€ DB helpers â”€â”€â”€â”€â”€
if USE_DB:
    conn = sqlite3.connect(SQLITE_PATH)
    conn.execute("CREATE TABLE IF NOT EXISTS sent(url TEXT PRIMARY KEY)")
else:
    conn = None

def already_sent(url: str) -> bool:
    if not conn:
        return False
    return conn.execute("SELECT 1 FROM sent WHERE url=?", (url,)).fetchone() is not None

def mark_sent(url: str):
    if conn:
        conn.execute("INSERT OR IGNORE INTO sent(url) VALUES (?)", (url,))
        conn.commit()

# â”€â”€â”€â”€â”€ COLLECT NEWS â”€â”€â”€â”€â”€

def newsapi_fetch(q: str) -> list[dict]:
    url = "https://newsapi.org/v2/everything"
    params = {
        "q": q,
        "language": "ru",
        "from": (dt.datetime.utcnow() - dt.timedelta(days=MAX_DAYS)).isoformat(),
        "pageSize": MAX_ARTICLES,
        "sortBy": "publishedAt",
        "domains": ",".join(WHITELIST),
        "apiKey": NEWS_API_KEY,
    }
    data = requests.get(url, params=params, timeout=12).json()
    return [{
        "title": a["title"],
        "url": a["url"],
        "date": a["publishedAt"][:10]
    } for a in data.get("articles", [])]


def rss_fetch(keywords: list[str]) -> list[dict]:
    res = []
    for feed in RSS_FEEDS:
        parsed = feedparser.parse(feed)
        for e in parsed.entries:
            title = e.get("title", "")
            if all(k in title.lower() for k in keywords):
                url = e.link
                res.append({"title": title, "url": url, "date": e.get("published", "")[:10]})
    return res


def gather_articles() -> list[dict]:
    topics = ["it", "Ñ€Ð¾ÑÑÐ¸Ð¹ÑÐºÐ¸Ð¹ it", "1Ñ"]
    pool = {}
    for t in topics:
        for art in newsapi_fetch(t) + rss_fetch(t.split()):
            if already_sent(art["url"]):
                continue
            pool[art["url"]] = art
            if len(pool) >= MAX_ARTICLES:
                break
    # ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð¿Ð¾ Ð´Ð°Ñ‚Ðµ
    articles = sorted(pool.values(), key=lambda a: a["date"], reverse=True)
    return articles[:MAX_ARTICLES]

# â”€â”€â”€â”€â”€ GPT PROMPT â”€â”€â”€â”€â”€

def build_prompt(articles: list[dict]) -> str:
    today = dt.datetime.now(TZ).strftime("%d %b %Y")
    json_blob = json.dumps(articles, ensure_ascii=False)
    return textwrap.dedent(f"""
        Ð¢Ñ‹Â â€” Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð½Ð¾Ð²Ð¾ÑÑ‚Ð½Ð¾Ð¹ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¾Ñ€.
        ÐÐ° Ð²Ñ…Ð¾Ð´ Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÑˆÑŒ JSON ÑÐ¾ ÑÑ‚Ð°Ñ‚ÑŒÑÐ¼Ð¸ (title, url, date).
        ÐžÑ‚Ð±ÐµÑ€Ð¸ {DIGEST_NEWS_CNT} ÑÐ°Ð¼Ñ‹Ñ… Ð·Ð½Ð°Ñ‡Ð¸Ð¼Ñ‹Ñ… Ð´Ð»Ñ Ñ€Ñ‹Ð½ÐºÐ° IT Ð Ð¤ Ð¸ ÑÐºÐ¾ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ 1Ð¡.
        Ð¡Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐ¹ Markdownâ€‘Ð´Ð°Ð¹Ð´Ð¶ÐµÑÑ‚:
        - ÐšÐ°Ð¶Ð´Ð°Ñ ÑÑ‚Ñ€Ð¾ÐºÐ°: "- **Ð—Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº** â€” 1 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ ÑÑƒÑ‚Ð¸. [Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº](url) (DD.MM.YYYY)".
        - Ð—Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°: "ðŸ—žï¸ **ITâ€‘Digest â€¢ {today}**".
        - Ð’ ÐºÐ¾Ð½Ñ†Ðµ Ð±Ð»Ð¾Ðº Â«ðŸ’¡ InsightÂ» â€” 2 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ Ð¾ Ð³Ð»Ð°Ð²Ð½Ð¾Ð¹ Ñ‚ÐµÐ½Ð´ÐµÐ½Ñ†Ð¸Ð¸.
        Ð’ÑÐµÐ³Ð´Ð° ÑÐ¾Ð±Ð»ÑŽÐ´Ð°Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚.
        JSON ÑÑ‚Ð°Ñ‚ÐµÐ¹: ```{json_blob}```
    """)

# â”€â”€â”€â”€â”€ BUILD DIGEST â”€â”€â”€â”€â”€

def build_digest(md_prompt: str) -> str:
    resp = client.chat.completions.create(
        model=MODEL,
        messages=[{"role": "user", "content": md_prompt}],
        temperature=0.4,
        max_tokens=1024,
    )
    return resp.choices[0].message.content.strip()

# â”€â”€â”€â”€â”€ TELEGRAM â”€â”€â”€â”€â”€

def send_tg(text: str):
    api = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
    for chunk in (text[i:i+3800] for i in range(0, len(text), 3800)):
        resp = requests.post(api, json={
            "chat_id": CHAT_ID,
            "text": chunk,
            "parse_mode": "MarkdownV2",
            "disable_web_page_preview": False,
        }, timeout=15)
        resp.raise_for_status()

# â”€â”€â”€â”€â”€ MAIN â”€â”€â”€â”€â”€

def main():
    arts = gather_articles()
    if len(arts) < DIGEST_NEWS_CNT:
        raise RuntimeError("ÐÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ ÑÐ²ÐµÐ¶Ð¸Ñ… ÑÑ‚Ð°Ñ‚ÐµÐ¹")
    md = build_digest(build_prompt(arts))
    send_tg(md)
    for a in arts[:DIGEST_NEWS_CNT]:
        mark_sent(a["url"])
    print("Digest sent âœ”ï¸Ž")

if __name__ == "__main__":
    main()
