#!/usr/bin/env python3
"""ITâ€‘Digest Telegram bot â€” v15.1Â (2025â€‘04â€‘22)

RSSâ€‘only + Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚ĞµĞ»Ğ°, Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ 1Ğ¡, HTMLâ€‘safe Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ°
"""
from __future__ import annotations

import os, re, json, datetime as dt, textwrap, requests, feedparser, html as _html
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from openai import OpenAI

# â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€
load_dotenv()
TG_TOKEN = os.getenv("TG_TOKEN")
CHAT_ID = os.getenv("CHAT_ID")
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
assert TG_TOKEN and CHAT_ID and OPENAI_KEY, "TG_TOKEN, CHAT_ID, OPENAI_API_KEY required"

MODEL = os.getenv("MODEL", "gpt-4o")
TZ = dt.timezone(dt.timedelta(hours=3))
MAX_DAYS = int(os.getenv("MAX_DAYS", 7))
DIGEST_NEWS_CNT = int(os.getenv("DIGEST_NEWS_CNT", 8))

client = OpenAI()

# â”€â”€â”€â”€â”€ KEYWORDS â”€â”€â”€â”€â”€
ONEC_DOMAINS = {"1c.ru", "infostart.ru", "odysseyconsgroup.com"}
ONEC_KEYS = {
    "1Ñ", "1c", "1â€‘Ñ", "1-Ñ", "1Ñ:erp", "1Ñ:Ğ¿Ñ€ĞµĞ´Ğ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ", "wms", "Ğ·ÑƒĞ¿",
    "ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ»ĞµĞ¹", "ÑƒÑ‚", "ÑƒĞ½Ñ„", "upp", "unf", "Ğ±ÑƒÑ…Ğ³Ğ°Ğ»Ñ‚ĞµÑ€Ğ¸Ñ", "odin es", "Ğ¾Ğ´Ğ¸Ğ½ÑÑ"
}
TECH_KEYWORDS = [
    "it", "ai", "Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½", "Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²", "Ğ¾Ğ±Ğ»Ğ°Ñ‡", "ĞºĞ¸Ğ±ĞµÑ€", "Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°Ñ",
    "erp", "crm", "wms", "1Ñ", "1c", "Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°", "Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†", "Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†",
    "Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½", "devops", "Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚", "api", "Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ", "kubernetes",
] + list(ONEC_KEYS)

# â”€â”€â”€â”€â”€ RSS LIST â”€â”€â”€â”€â”€
RSS_FEEDS = [
    "https://habr.com/ru/rss/all/all/?fl=ru",
    "https://vc.ru/rss",
    "https://www.rbc.ru/technology/rss/full/",
    "https://tadviser.ru/index.php/Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ:ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸?feed=rss",
    "https://novostiitkanala.ru/feed/",
    "https://www.kommersant.ru/RSS/section-tech.xml",
    "https://1c.ru/news/all.rss",
    "https://infostart.ru/rss/news/",
    "https://trends.rbc.ru/trends.rss",
    "https://rusbase.com/feed/",
]

CUTOFF = dt.datetime.utcnow() - dt.timedelta(days=MAX_DAYS)

# â”€â”€â”€â”€â”€ FETCH & FILTER â”€â”€â”€â”€â”€

def _plain(html: str) -> str:
    return BeautifulSoup(html, "html.parser").get_text(" ").lower()

def _hit(text: str) -> bool:
    return any(k in text for k in TECH_KEYWORDS)

def rss_fetch() -> list[dict]:
    out = []
    for url in RSS_FEEDS:
        try:
            feed = feedparser.parse(url)
        except Exception:
            continue
        for e in feed.entries:
            link, title = e.get("link", ""), e.get("title", "")
            date_str = e.get("published", "")[:10]
            try:
                date_obj = dt.datetime.strptime(date_str, "%Y-%m-%d")
            except Exception:
                date_obj = CUTOFF
            if date_obj >= CUTOFF:
                out.append({"title": title, "url": link, "date": date_str})
    return sorted(out, key=lambda a: a["date"], reverse=True)

def filter_stage(arts: list[dict]) -> list[dict]:
    first = [a for a in arts if _hit(a["title"].lower())]
    if len(first) >= DIGEST_NEWS_CNT:
        return first[:DIGEST_NEWS_CNT]
    for a in arts:
        if a in first:
            continue
        try:
            page = requests.get(a["url"], timeout=10).text
        except Exception:
            continue
        if _hit(_plain(page)):
            first.append(a)
        if len(first) >= DIGEST_NEWS_CNT:
            break
    return first

def is_onec(a: dict) -> bool:
    return (any(k in a["title"].lower() for k in ONEC_KEYS) or
            urlparse(a["url"]).netloc in ONEC_DOMAINS)

def select_articles(all_arts: list[dict]) -> list[dict]:
    arts = filter_stage(all_arts)
    onec = [a for a in arts if is_onec(a)]
    other = [a for a in arts if not is_onec(a)]
    sel = (onec[:2] if len(onec) >= 2 else onec) + other
    return sel[:DIGEST_NEWS_CNT]

# â”€â”€â”€â”€â”€ GPT PROMPT â”€â”€â”€â”€â”€

def build_prompt(arts: list[dict]) -> str:
    today = dt.datetime.now(TZ).strftime("%d %b %Y")
    return textwrap.dedent(f"""
        ĞĞ° Ğ²Ñ…Ğ¾Ğ´Ğµ JSON ÑÑ‚Ğ°Ñ‚ĞµĞ¹ (title, url, date). Ğ¡Ğ¾ÑÑ‚Ğ°Ğ²ÑŒ Ğ´Ğ°Ğ¹Ğ´Ğ¶ĞµÑÑ‚ HTMLâ€‘Markdown Ñ Ñ‚Ñ€ĞµĞ¼Ñ ÑĞµĞºÑ†Ğ¸ÑĞ¼Ğ¸:
        ğŸŒ <b>GLOBALÂ IT</b>\nğŸ‡·ğŸ‡º <b>RUÂ TECH</b>\nğŸŸ¡ <b>1Ğ¡Â Ğ­ĞšĞĞ¡Ğ˜Ğ¡Ğ¢Ğ•ĞœĞ</b>
        Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚: "- <b>Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº</b> â€” 1â€‘2 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ. <a href=\"url\">Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº</a> (DD.MM.YYYY)".
        Ğ•ÑĞ»Ğ¸ ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¼ĞµĞ½ÑŒÑˆĞµ {DIGEST_NEWS_CNT} â€” Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸ ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞµÑÑ‚ÑŒ.
        Ğ’ ĞºĞ¾Ğ½Ñ†Ğµ Ğ±Ğ»Ğ¾Ğº "ğŸ’¡ <b>Insight</b>:" â€” 2 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ.
        JSON: ```{json.dumps(arts, ensure_ascii=False)}```
    """).strip()

def build_digest(prompt: str) -> str:
    resp = client.chat.completions.create(
        model=MODEL,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.4,
        max_tokens=1000,
    )
    return resp.choices[0].message.content.strip()

# â”€â”€â”€â”€â”€ TELEGRAM â”€â”€â”€â”€â”€

def _sanitize(html_txt: str) -> str:
    html_txt = re.sub(r'href="([^"]+)"', lambda m: f'href="{m.group(1).replace("&", "&amp;")}"', html_txt)
    parts = re.split(r'(<[^>]+>)', html_txt)
    return ''.join(p if p.startswith('<') else _html.escape(p) for p in parts)

def send_telegram(html: str):
    api = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
    for i in range(0, len(html), 3800):
        chunk = _sanitize(html[i:i+3800])
        r = requests.post(api, json={
            "chat_id": CHAT_ID,
            "text": chunk,
            "parse_mode": "HTML",
            "disable_web_page_preview": False,
        }, timeout=20)
        print("TG", r.status_code, r.text[:80])
        r.raise_for_status()

# â”€â”€â”€â”€â”€ MAIN â”€â”€â”€â”€â”€

def main():
    arts = select_articles(rss_fetch())
    digest = build_digest(build_prompt(arts))
    send_telegram(digest)
    print(f"Digest sent with {len(arts)} articles âœ”ï¸")

if __name__ == "__main__":
    main()
